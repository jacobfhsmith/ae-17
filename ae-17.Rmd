---
title: "AE 18: Simulation-based inference review"
date: "October 20, 2021"
author: "STA 199"
---

```{r setup, include = F}
knitr::opts_chunk$set(warning =  FALSE, 
                      message = FALSE)
```


```{r load-packages}
library(tidyverse)
library(tidymodels)
```

```{r load-data}
yawn <- read_csv("yawn.csv")
```

# Learning goals

- Understand and apply simulation-based methods to test a claim about independence between two groups
- Understand and apply simulation-based methods to calculate confidence interval to estimate difference in proportions
- Review simulation-based methods

# Part 1: Evaluating independence

We will use data from the *Mythbusters* experiment to determine if yawning is contagious.  Let $t$ be the treatment group who saw a person yawn, $c$ be the control group who did not see anyone yawn , and $p$ be the proportion of people who yawned. The observed statistic from the *Mythbusters* episode is $\hat{p}_t - \hat{p}_c  = 0.0441$.

We want to use simulation-based inference to assess whether or not yawning and seeing someone yawn are independent. The hypotheses in mathematical notation are

$$H_0: p_t = p_c \text{ vs. }H_a: p_t \neq p_c$$

## Exercise 1

- Fill in the code below to generate the null distribution. Uncomment the code once it is complete.

```{r mythbusters-null}
set.seed(101821)
#null_dist <- yawn %>%
#  specify(response = ____, explanatory = _____, success = "yawn") %>%
#  hypothesize(null = "______") %>%
#  generate(100, type = "permute") %>%
#  calculate(stat = "______", 
#            order = c("trmt", "ctrl"))
```

- Visualize the null distribution and shade in the area used to calculate the p-value. 

```{r mythbusters-null-viz}
# add code 
```

- Calculate p-value. Then use the p-value to make your conclusion using a significance level of 0.1.

```{r mythbusters-calc-p-value}
# add code
```

## Exercise 2: Confidence interval 

Construct a 90% confidence interval for the difference in proportion of yawners between those who see someone else yawn and those who don't. 

```{r}
#boot_dist <- yawn %>%
#  specify(response = ____, explanatory = _____, success = "yawn") %>%
#  generate(100, type = "bootstrap") %>%
#  calculate(stat = "diff in props", 
#            order = c("trmt", "ctrl"))
```

- Why are we using "bootstrap" instead of "permute" here? 

```{r}
# calculate the lower and upper bounds for the 90% ci
```

- Interpret the interval in the context of the data. 

- Suppose you use the confidence interval to evaluate the hypotheses in Exercise 1. Is the conclusion drawn from the confidence interval consistent with the conclusion from the the hypothesis test?

# Part 2: Describing simulation-based inference

The members of the Statistics Department randomly sample 25 majors and find that 15 of the students work 5 or more hours each week. Use the code below to create a data frame of the results.

```{r create-df}
stats_work <- tibble(work_hours = c(rep("At least 5", 15), 
                               rep("Less than 5", 10)))
```

We would like to calculate a 95% bootstrap confidence interval for the proportion of full-time college students who work at least 5 hours per week. 

Describe how the bootstrap distribution for the proportion of full-time college students who work at least 5 hours per week is generated. In your description, you can imagine using blue and red marbles to represent the data. Your description should also include specifics about the size of the sample drawn at each iteration and what statistic is calculated. You can assume the number of reps for the simulation is 10,000.

## Intro to CLT (if time)

# Variability of sample statistics

- Each sample from the population yields a slightly different sample 
  statistic (sample mean, sample proportion, etc.)

- The variability of these sample statistics is measured by the **standard error**

- Previously we quantified this value via simulation

- Today we'll discuss some of the theory underlying **sampling distributions**, particularly as they relate to *sample means*.

## Recall

Statistical inference is the act of generalizing from a sample in order to make conclusions regarding a population. As part of this process, we quantify the degree of certainty we have. 

We are interested in population parameters, which we do not observe. Instead, we must calculate statistics from our sample in order to learn about them.

## Sampling distribution of the mean

Suppose weâ€™re interested in the resting heart rate of students at Duke, and are able to do the following:

1. Take a random sample of size $n$ from this population, and calculate the 
   mean resting heart rate in this sample, $\bar{X}_1$

2. Put the sample back, take a second random sample of size $n$, and calculate the mean resting heart rate from this new sample, $\bar{X}_2$

3. Put the sample back, take a third random sample of size $n$, and calculate the mean resting heart rate from this sample, too...and so on.

After repeating this many times, we have a dataset that has the sample averages from the population: $\bar{X}_1$, $\bar{X}_2$, $\cdots$,
$\bar{X}_K$ (assuming we took $K$ total samples).

## Sampling distribution of the mean

**Question**: Can we say anything about the distribution of these sample means?
*(Keep in mind, we don't know what the underlying distribution of mean resting heart rate looks like in Duke students!)*

As it turns out, we can...

## The Central Limit Theorem

For a population with a well-defined mean $\mu$ and standard deviation $\sigma$, these three properties hold for the distribution of sample average $\bar{X}$,assuming certain conditions hold:

1. The mean of the sampling distribution is identical to the population mean
$\mu$,

2. The standard deviation of the distribution of the sample averages is
$\sigma/\sqrt{n}$, or the **standard error** (SE) of the mean, and

3. For $n$ large enough (in the limit, as $n \to \infty$), the shape of the
sampling distribution of means is approximately *normal* (Gaussian).

## What is the normal (Gaussian) distribution?

The normal distribution is unimodal and symmetric and is described by its
*density function*:

If a random variable $X$ follows the normal distribution, then
$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{ -\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2} \right\}$$
where $\mu$ is the mean and $\sigma^2$ is the variance. 

We often write $N(\mu, \sigma^2)$ to describe this distribution.

## The normal distribution (graphically)

We will talk about probability densities and using them to define probabilities later, but for now, just know that the normal 
distribution is the familiar "bell curve":

```{r normdist, echo = FALSE, fig.height=5}
x <- seq(-3, 3, 0.01)
y <- dnorm(x)
plot(x, y, 
     type = "l", 
     lwd = 2,
     xlab = "", 
     ylab = "Density, N(0, 1)")
```

## But we didn't know anything about the underlying distribution!

The central limit theorem tells us that sample averages are
normally distributed, if we have enough data. This is true even if our original variables are not normally distributed.

[**Check out this interactive demonstration!**](http://onlinestatbook.com/stat_sim/sampling_dist/index.html)

## Conditions

What are the conditions we need for the CLT to hold?

- **Independence:** The sampled observations must be independent. This is 
difficult to check, but the following are useful guidelines:
    - the sample must be random
    - if sampling without replacement, sample size must be less than 10% of the population size
    
- **Sample size / distribution:** 
    - if data are numerical, usually n $\geq$ 30 is considered a large enough sample, but if the underlying population distribution is extremely skewed, more might be needed
    - if we know for sure that the underlying data are normal, then the 
    distribution of sample averages will also be exactly normal, regardless of
    the sample size
    - if data are categorical, at least 10 successes and 10 failures.

*More on this next class.*